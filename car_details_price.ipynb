{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ratnesh1210/Car-details-price-prediction/blob/main/car_details_price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Car Price Prediction Using Regression model"
      ],
      "metadata": {
        "id": "wh2hreVP3_Nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - regression\n",
        "##### **Contribution**    - TIndividual\n",
        "##### **Cntributor -** - Ratnesh Verma\n"
      ],
      "metadata": {
        "id": "lScro1gQ4PPn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "9_ThoPfp44VF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset comprises a wealth of information regarding used cars, including details such as the year in which the car was manufactured, the selling price, the amount of kilometers driven, the type of fuel used, the type of seller (whether a dealer or an individual), the type of transmission (automatic or manual), and the number of previous owners. \n",
        "\n",
        "the task at hand is to apply various machine learning regression models to the provided used car dataset, with the objective of accurately predicting the selling price of the cars based on the available features. This could involve using multiple linear regression, polynomial regression, decision tree regression, random forest regression, or other suitable regression models. The ultimate goal is to identify the model or combination of models that yields the most accurate and reliable price predictions for the used cars in the dataset.\n"
      ],
      "metadata": {
        "id": "4nfXq3cL2DTc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "4mbK17dx5Bt2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement**\n"
      ],
      "metadata": {
        "id": "Js8Uv_9A5Fzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the task at hand is to apply various machine learning regression models to the provided used car dataset, with the objective of accurately predicting the selling price of the cars based on the available features. This could involve using multiple linear regression, polynomial regression, decision tree regression, random forest regression, or other suitable regression models. The ultimate goal is to identify the model or combination of models that yields the most accurate and reliable price predictions for the used cars in the dataset."
      ],
      "metadata": {
        "id": "JoOiKafh6OuD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "oRk84paX5Lho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "CO4DR47-61Ne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FJjusfBnOuW"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "import warnings    \n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnaqS7hFmH9y"
      },
      "outputs": [],
      "source": [
        "#mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIH92z6TtlL-"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p93CbieCkVqm"
      },
      "outputs": [],
      "source": [
        "#Importing data set\n",
        "df =pd.read_csv(\"/content/drive/MyDrive/CAR DETAILS (1).csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TFVpATvnK3Z"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JopB3U81tpgN"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzGOshuOtvle"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "print('The number of rows in data is:',df.shape[0])\n",
        "print('The number of columns in data is',len(list(df.columns)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQkqWisBt0O-"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWyd58bAt141"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7irSj-mOuCYY"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnsxP2RruDcn"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# df.duplicated().sum()\n",
        "duplicate_rows_in_store_data = df.duplicated().sum()\n",
        "print('The number of duplicates in store data is:',duplicate_rows_in_store_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1EMK8ZUuRrV"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NP367zOt-6e"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjQFUDnduok-"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A69tlmKHuusu"
      },
      "source": [
        "1. -There is no null value present in the data set.\n",
        "2. -There are 8 columns in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lQDI0QkuvoV"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpaGd0EouqGO"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "print(list(df.columns))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking information about the data type of the variable\n",
        "df.info()"
      ],
      "metadata": {
        "id": "fErLQCddLTQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNGrZWRwu1uN"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY6Pa_SxvKAG"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2sWw3LuvCgn"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "for col in df.columns:\n",
        "  print(f'The unique values in {col} are {df[col].unique()}' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJVzjfvx2MVy"
      },
      "source": [
        "## 3. ***Data Wrangling and visualization***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5zjfeGMKnVA"
      },
      "outputs": [],
      "source": [
        "# Get the counts of each category in the column\n",
        "counts = df['fuel'].value_counts()\n",
        "\n",
        "# Create a pie chart\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "plt.pie(counts.values, labels=counts.index, autopct='%1.1f%%' , startangle=45 , textprops={'fontsize': 10})\n",
        "plt.axis('equal')\n",
        "plt.title('Car_type')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh7UDfAM2yOM"
      },
      "outputs": [],
      "source": [
        "unique_car = df[\"name\"].nunique()\n",
        "print(\"The number of unique car count variables is:\", unique_car)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XxcI08LLwZi"
      },
      "outputs": [],
      "source": [
        "car_owner = df.groupby(\"owner\")[\"name\"].count().reset_index()\n",
        "car_owner\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtZAdq3UJehr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(13,8))\n",
        "plt.title('car_owner Type')\n",
        "sns.barplot(x='owner',y='name',data=car_owner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5J7b0GyVKdW9"
      },
      "outputs": [],
      "source": [
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4bMejwWPD32"
      },
      "outputs": [],
      "source": [
        "selling_price = df.groupby(\"fuel\")[\"selling_price\"].sum().reset_index()\n",
        "selling_price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPCYDeYLQFjH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(13,8))\n",
        "plt.title('max_sell_CAR_Type')\n",
        "sns.barplot(x='fuel',y='selling_price',data=selling_price)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXPuqXY3QzJE"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code\n",
        "newlist=df['owner'].unique()\n",
        "y=list(newlist)\n",
        "# Applying for loop operation\n",
        "for x in y:\n",
        "  sub_1=df.loc[df['owner'] == x]\n",
        "  p_sub1=sub_1.groupby(['owner'])['selling_price'].value_counts().head(3)\n",
        "  print(p_sub1)\n",
        "  plt.figure(figsize=(5,3))\n",
        "  p_sub1.plot(kind='bar')\n",
        "  plt.title('Top 3 sold car of '+str(x))\n",
        "  plt.xlabel(\"car\")\n",
        "  plt.ylabel(\"Counting\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qs6mWLOWup-"
      },
      "outputs": [],
      "source": [
        "df_plot=df.groupby(['seller_type'])['name'].count().reset_index()\n",
        "df_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nx0kEkUkGD7I"
      },
      "outputs": [],
      "source": [
        "#visualization code\n",
        "#Writing a code for plotting line plot between the target variable and age, cigsPerDay, and heartRate\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x='km_driven', y=\"selling_price\", data=df.head(200))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqKmuiVOa-4f"
      },
      "source": [
        "## ***3. Feature Engineering & Data Pre-processing***\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rZnK3DHtOZu"
      },
      "outputs": [],
      "source": [
        "df1 = df [['year', 'selling_price', 'km_driven',\n",
        "       'fuel', 'seller_type', 'transmission', 'owner']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kvPr3m9jV9C"
      },
      "outputs": [],
      "source": [
        "df1['Current_Year'] = 2022"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi6j_l0sjrIL"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05ckY792jwzO"
      },
      "outputs": [],
      "source": [
        "#Creating our new column no_of_year\n",
        "df1['Year_old'] = df1['Current_Year'] - df1['year']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DdXY_6Jkm3Y"
      },
      "outputs": [],
      "source": [
        "#One Hot Encoding for Categorical variables by creating dummy variables\n",
        "df1 = pd.get_dummies(df1, drop_first = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfpBkBHEu0YC"
      },
      "outputs": [],
      "source": [
        "df1.drop(['year','Current_Year'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Spln_pRu_8F"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmf_CokoZDkn"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mARcH0e0F02K"
      },
      "outputs": [],
      "source": [
        "#plotting distribution plot of target Variables\n",
        "sns.distplot(x=df1.selling_price)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_1znpevd7wc"
      },
      "outputs": [],
      "source": [
        "#plotting distribution plot of target Variables\n",
        "sns.distplot(x=df1.km_driven)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCNtri6GZ8fr"
      },
      "source": [
        "A Distplot or distribution plot, depicts the variation in the data distribution. Seaborn Distplot represents the overall distribution of continuous data variables i.e. data distribution of a variable against the density distribution. In above graph we can see that our graph have right tail(right skewed) it means tha our most of data centered near peak of the graph but there is a some highly expensive price that leads to a seperate trends that's why our graph showing right trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXq5iDUXhvja"
      },
      "outputs": [],
      "source": [
        "\n",
        "# calculate the upper and lower limits for the km_driven column using the capping method\n",
        "Q1 = df1[\"km_driven\"].quantile(0.25)\n",
        "Q3 = df1[\"km_driven\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "upper_limit = Q3 + 1.5*IQR\n",
        "lower_limit = Q1 - 1.5*IQR\n",
        "\n",
        "# replace outliers above the upper limit with the nearest non-outlier value, and outliers below the lower limit with the nearest non-outlier value\n",
        "df1[\"km_driven\"] = np.where(df1[\"km_driven\"] > upper_limit, df1[\"km_driven\"].quantile(0.95), df1[\"km_driven\"])\n",
        "df1[\"km_driven\"] = np.where(df1[\"km_driven\"] < lower_limit, df1[\"km_driven\"].quantile(0.05), df1[\"km_driven\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4ACsceSregF"
      },
      "source": [
        "In above, the upper and lower limits are calculated using the interquartile range (IQR) multiplied by 1.5. Any values above the upper limit or below the lower limit are replaced with the 99th or 1st percentile value, respectively, using the NumPy where function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZ91fHyjfMUW"
      },
      "outputs": [],
      "source": [
        "sns.distplot(x=df1.km_driven)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3SIrlJiiVS2"
      },
      "outputs": [],
      "source": [
        "df1[\"km_driven_sqrt\"] = np.sqrt(df1[\"km_driven\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eIw7dlkiY3H"
      },
      "outputs": [],
      "source": [
        "sns.distplot(x=df1.km_driven_sqrt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5miW3JNVcwHr"
      },
      "source": [
        "after transforming the data now we can see that our data looks good and Logarithmic transformation converted our data to a normal distribution. It is a type of data transformation that can be applied to data that has a wide range of values or is skewed in one direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDMRcUBUWbV3"
      },
      "outputs": [],
      "source": [
        "# take the natural logarithm of a column called \"column_to_transform\"\n",
        "df1[\"selling_price_lg\"] = np.log(df1[\"selling_price\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajkgsTHRWm8w"
      },
      "outputs": [],
      "source": [
        "#plotting distribution plot of target Variables\n",
        "sns.distplot(x=df1.selling_price_lg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRtVZIdHWidz"
      },
      "source": [
        "Logarithmic transformation is a mathematical operation used in data analysis and modeling to convert data that is not normally distributed to a normal distribution. It is a type of data transformation that can be applied to data that has a wide range of values or is skewed in one direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56G1gwAxrwRx"
      },
      "outputs": [],
      "source": [
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CETuzHHbtVLE"
      },
      "outputs": [],
      "source": [
        "#dropping un_necessary colmns\n",
        "df1.drop(['selling_price','km_driven'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoD-cf8PUflp"
      },
      "outputs": [],
      "source": [
        "df1.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-5D5IiPVWUA"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmoHQ4gOMuxN"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# define your feature matrix X and target variable y\n",
        "X = df1.drop('selling_price_lg', axis=1)\n",
        "y = df1['selling_price_lg']\n",
        "\n",
        "# train a Random Forest regressor\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(X, y)\n",
        "\n",
        "# get feature importances and put them into a DataFrame\n",
        "importances = pd.DataFrame({'feature': X.columns, 'importance': rf.feature_importances_})\n",
        "\n",
        "# sort the DataFrame by importance (descending)\n",
        "importances = importances.sort_values('importance', ascending=False)\n",
        "\n",
        "# print the top 10 features by importance\n",
        "print(importances.head(16))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMUv2x4lUYpO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# create a bar chart of feature importances\n",
        "plt.bar(importances['feature'], importances['importance'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Importance')\n",
        "plt.xlabel('Feature')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqr-N7fcU2CC"
      },
      "outputs": [],
      "source": [
        "#removing un-necessary features\n",
        "df1.drop(['fuel_LPG', 'owner_Test Drive Car','fuel_Electric' ], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_X_eVmwWO2B"
      },
      "outputs": [],
      "source": [
        "df1.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHADttELXx9O"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQwpYQTPh93v"
      },
      "source": [
        "StandardScaler is a popular method for scaling numerical data, typically used in machine learning and data analysis. It transforms your data so that it has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "StandardScaler is a commonly used scaling method that scales the data to have zero mean and unit variance. This ensures that the features are on the same scale and have similar ranges. Scaling is generally considered a good practice in machine learning and data analysis, and is often a necessary step in the preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yElGFGESX9TS"
      },
      "outputs": [],
      "source": [
        "#making copy\n",
        "df2=df1.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-5oKCYdYE6P"
      },
      "outputs": [],
      "source": [
        "#importing libraray\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKfm-rCbaWMy"
      },
      "outputs": [],
      "source": [
        "#convering data type\n",
        "df2['Year_old'] = df2['Year_old'].astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFR-mu2hYIq-"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "#applying standardScaler \n",
        "scaler = StandardScaler()\n",
        "df3 = scaler.fit_transform(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4okfEs_g0N7"
      },
      "outputs": [],
      "source": [
        "#converting back to dataframe\n",
        "df3 = pd.DataFrame(df3, columns=df2.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cYyXGGmgQYk"
      },
      "outputs": [],
      "source": [
        "df3.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKrZRyZBioWB"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n"
      ],
      "metadata": {
        "id": "H84IlC-Xw9_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6Rjicxri9eS"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "x, y = df3.loc[:, df3.columns != 'selling_price_lg'], df3['selling_price_lg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW_XUONJuAqC"
      },
      "outputs": [],
      "source": [
        "#importing library to split\n",
        "from sklearn.model_selection import train_test_split\n",
        "#dividing the data for training and testing\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state = 0)\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cGMvE6B1iFNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***4. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1DkwNEcdmkH"
      },
      "outputs": [],
      "source": [
        "# create a dataframe to store metrics related to models\n",
        "metrics_table = pd.DataFrame(columns=['Regression_Model', 'Train_R2', 'Test_R2', 'Train_RMSE', 'Test_RMSE', 'Train_RMSPE', 'Test_RMSPE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp-j2OO4d2Ez"
      },
      "outputs": [],
      "source": [
        "# define a function to calculate root mean squared percentage error\n",
        "# returns an array\n",
        "def calculate_rmspe(y, y_pred):\n",
        "  return (np.sqrt(np.mean(np.square(y.to_numpy() - y_pred))) / np.mean(y.to_numpy())) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIhSdi45d4GT"
      },
      "outputs": [],
      "source": [
        "# define a function to calculate metrics\n",
        "# returns a dictionary\n",
        "def calculate_model_metrics(y_train, y_train_pred, y_test, y_test_pred):\n",
        "  metrics_dict = {}\n",
        "\n",
        "  metrics_dict['Train_R2'] = r2_score(y_train, y_train_pred)\n",
        "  metrics_dict['Test_R2'] = r2_score(y_test, y_test_pred)\n",
        "  metrics_dict['Train_RMSE'] = mean_squared_error(y_train, y_train_pred, squared=False)\n",
        "  metrics_dict['Test_RMSE'] = mean_squared_error(y_test, y_test_pred, squared=False)\n",
        "  metrics_dict['Train_RMSPE'] = calculate_rmspe(y_train, y_train_pred)\n",
        "  metrics_dict['Test_RMSPE'] = calculate_rmspe(y_test, y_test_pred)\n",
        "\n",
        "  return metrics_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1 Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "# Fit the Algorithm\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSt7I_6J_izA"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regression_model = LinearRegression()\n",
        "regression_model.fit(x_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcuFdqH6CJ1Q"
      },
      "outputs": [],
      "source": [
        "regression_model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4kldYVascK3"
      },
      "outputs": [],
      "source": [
        "#checking prediction\n",
        "y_train_pred = regression_model.predict(x_train)\n",
        "y_test_pred = regression_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaeQJeAgeL-W"
      },
      "outputs": [],
      "source": [
        "#calculating for model metrics\n",
        "model_evaluation = calculate_model_metrics(y_train, y_train_pred, y_test, y_test_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vScWpS32ePdF"
      },
      "outputs": [],
      "source": [
        "#storing data in metric tables\n",
        "metrics_table.loc[len(metrics_table.index)] = ['Linear', model_evaluation['Train_R2'], model_evaluation['Test_R2'], \n",
        "                                                         model_evaluation['Train_RMSE'], model_evaluation['Test_RMSE'], \n",
        "                                                         model_evaluation['Train_RMSPE'], model_evaluation['Test_RMSPE']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGrWcAk3CRIK"
      },
      "source": [
        "We have used Linear Regression as our first model, Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQ6v-f0sKMUu"
      },
      "outputs": [],
      "source": [
        "metrics_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "\n",
        "# cross validation using k fold technique\n",
        "from sklearn.model_selection import cross_val_score\n",
        "for i in [3,5,10]:\n",
        "  score = cross_val_score(LinearRegression(), x_train, y_train,cv=i)\n",
        "  print(np.average(score))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hyper parameter tuning using RandomizedSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "lr = LinearRegression()\n",
        "\n",
        "param_distributions = {\"fit_intercept\": [True, False],\n",
        "                        \"copy_X\": [True, False],\n",
        "                       \"positive\": [True, False]}\n",
        "search = RandomizedSearchCV(lr, param_distributions).fit(x, y)\n",
        "search.best_score_ "
      ],
      "metadata": {
        "id": "Uf3AQd_5vqef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbPjmGVZYE5f"
      },
      "outputs": [],
      "source": [
        "c=search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model on the entire training data using best hyperparameters\n",
        "lr = LinearRegression(fit_intercept=c[\"fit_intercept\"], copy_X=c[\"copy_X\"], positive=c[\"positive\"])\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "# Predict the target variable for test data using the trained model\n",
        "y_pred = lr.predict(x_test)\n",
        "\n",
        "# Evaluate the model performance on test data\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "\n",
        "print(\"R^2 score:\", r2_score(y_test, y_pred))\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "n6df53JqvdJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "Over here we have used **RandomizedSearchCV** hyperparameter optimization technique , the reason for using this technique is that In order to train and score the model, Random Search creates a grid of hyperparameter values and chooses random combinations. As a result, we are able to specifically regulate the quantity of parameter combinations that are tried. Based on available time or resources, the number of search iterations is decided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "We have seen slight fall in the accuracy of the model by using the hyperparameter tuning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "###ML Model - 2 Decision Tree Regressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "decision_tree_model = DecisionTreeRegressor()\n",
        "decision_tree_model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_gGJblmEsDW"
      },
      "outputs": [],
      "source": [
        "decision_tree_model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMEtfwWEgLH7"
      },
      "outputs": [],
      "source": [
        "# predict the train and test data\n",
        "y_train_pred = decision_tree_model.predict(x_train)\n",
        "y_test_pred = decision_tree_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KlOPLI-gS4V"
      },
      "outputs": [],
      "source": [
        "model_evaluation = calculate_model_metrics(y_train, y_train_pred, y_test, y_test_pred)\n",
        "\n",
        "metrics_table.loc[len(metrics_table.index)] = ['Decision Tree', model_evaluation['Train_R2'], model_evaluation['Test_R2'], \n",
        "                                                         model_evaluation['Train_RMSE'], model_evaluation['Test_RMSE'], \n",
        "                                                         model_evaluation['Train_RMSPE'], model_evaluation['Test_RMSPE']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88y50_SSFR6Y"
      },
      "source": [
        "Over here we have used Decision tree regressor, Decision tree regression observes features of an object and trains a model in the structure of a tree to predict data in the future to produce meaningful continuous output. Continuous output means that the output/result is not discrete, i.e., it is not represented just by a discrete, known set of numbers or values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_nvtWKEKg6A"
      },
      "outputs": [],
      "source": [
        "metrics_table.loc[1:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm5corBHgonX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score\n",
        "from pylab import rcParams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "# cross validation using k fold technique\n",
        "from sklearn.model_selection import cross_val_score\n",
        "for i in [3,5,10]:\n",
        "  score = cross_val_score(DecisionTreeRegressor(), X, y,cv=i)\n",
        "  print(np.mean(score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W1PfiIhE1Au"
      },
      "outputs": [],
      "source": [
        "#hyper parameter tuning using RandomizedSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "dtr = DecisionTreeRegressor()\n",
        "\n",
        "param_distributions = {\"criterion\": [\"squared_error\", \"poisson\",\"friedman_mse\" ],\n",
        "                        \"splitter\": [\"best\", \"random\"],\n",
        "                       \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \"max_depth\" : [10]}\n",
        "search = RandomizedSearchCV(dtr, param_distributions).fit(X, y)\n",
        "search.best_score_ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPYPjll1Lf12"
      },
      "outputs": [],
      "source": [
        "search.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "Over here we have used RandomizedSearchCV hyperparameter optimization technique , the reason for using this technique is that In order to train and score the model, Random Search creates a grid of hyperparameter values and chooses random combinations. As a result, we are able to specifically regulate the quantity of parameter combinations that are tried. Based on available time or resources, the number of search iterations is decided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "We have seen slight fall in the accuracy of the model by using the hyperparameter tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "Evaluation metrics are used in machine learning to measure the performance of a model on a given dataset. This allows us to compare the performance of different models and select the one that performs the best on the task at hand. we have used evaluation metrics for this tasks, mean squared error for regression problems , RMSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHaT7Aw5FwrK"
      },
      "source": [
        "###ML Model - 3 Random Forest Regressor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RZ7KSodzaGlB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES3OZYRIGQYr"
      },
      "source": [
        "####1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6kPzEcTF0lq"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "random_forest_model = RandomForestRegressor()\n",
        "random_forest_model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2djhduWF6f5"
      },
      "outputs": [],
      "source": [
        "random_forest_model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkTd5U64ihXA"
      },
      "outputs": [],
      "source": [
        "# predict the train and test data\n",
        "y_train_pred = random_forest_model.predict(x_train)\n",
        "y_test_pred = random_forest_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44wzKDrtirAe"
      },
      "outputs": [],
      "source": [
        "# model evaluation\n",
        "model_evaluation = calculate_model_metrics(y_train, y_train_pred, y_test, y_test_pred)\n",
        "\n",
        "# add metrics to metrics table\n",
        "metrics_table.loc[len(metrics_table.index)] = ['Random Forest', model_evaluation['Train_R2'], model_evaluation['Test_R2'], \n",
        "                                                                model_evaluation['Train_RMSE'], model_evaluation['Test_RMSE'], \n",
        "                                                                model_evaluation['Train_RMSPE'], model_evaluation['Test_RMSPE']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SKblYRtGAj6"
      },
      "source": [
        "A supervised learning technique called Random Forest Regression leverages the ensemble learning approach for regression. The ensemble learning method combines predictions from various machine learning algorithms to provide predictions that are more accurate than those from a single model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDvRsx0CMuwW"
      },
      "outputs": [],
      "source": [
        "metrics_table.loc[2:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 4 Lasso and Ridge Regression (L1 and L2 Regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
        "lasso_model = Lasso()\n",
        "lasso_model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsg6pPYuoUh-"
      },
      "outputs": [],
      "source": [
        "lasso_model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kub8Z6H1Ycva"
      },
      "outputs": [],
      "source": [
        "#ridge REgression\n",
        "ridge_model = Ridge()\n",
        "ridge_model.fit(x_train, y_train)\n",
        "ridge_model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLoChJUnkYK_"
      },
      "outputs": [],
      "source": [
        "# predict the train and test data\n",
        "y_train_pred = ridge_model.predict(x_train)\n",
        "y_test_pred = ridge_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zx7h0IHklt1"
      },
      "outputs": [],
      "source": [
        "# model evaluation\n",
        "model_evaluation = calculate_model_metrics(y_train, y_train_pred, y_test, y_test_pred)\n",
        "\n",
        "# add metrics to metrics table\n",
        "metrics_table.loc[len(metrics_table.index)] = ['Ridge', model_evaluation['Train_R2'], model_evaluation['Test_R2'], \n",
        "                                                                model_evaluation['Train_RMSE'], model_evaluation['Test_RMSE'], \n",
        "                                                                model_evaluation['Train_RMSPE'], model_evaluation['Test_RMSPE']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGAT6Az9Yflc"
      },
      "outputs": [],
      "source": [
        "#elasticnet regression\n",
        "elasticnet_model = ElasticNet()\n",
        "elasticnet_model.fit(x_train, y_train)\n",
        "elasticnet_model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NTpXf9_lMay"
      },
      "source": [
        "Evaluation metrics are used in machine learning to measure the performance of a model on a given dataset. This allows us to compare the performance of different models and select the one that performs the best on the task at hand. we have used evaluation metrics for this tasks, mean squared error for regression problems , RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze4_m4zNOmnZ"
      },
      "outputs": [],
      "source": [
        "metrics_table.loc[3:,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PIHJqyupx6M"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J4cN8T_lMRp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "lasso_regressor.fit(x_train, y_train)\n",
        "# Predict on the model\n",
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hODqT4OEmxsD"
      },
      "outputs": [],
      "source": [
        "# predict the train and test data\n",
        "y_train_pred = lasso_regressor.predict(x_train)\n",
        "y_test_pred = lasso_regressor.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzNUbJ4Nmywt"
      },
      "outputs": [],
      "source": [
        "# model evaluation\n",
        "model_evaluation = calculate_model_metrics(y_train, y_train_pred, y_test, y_test_pred)\n",
        "# add metrics to metrics table\n",
        "metrics_table.loc[len(metrics_table.index)] = ['Lasso', model_evaluation['Train_R2'], model_evaluation['Test_R2'], \n",
        "                                                        model_evaluation['Train_RMSE'], model_evaluation['Test_RMSE'], \n",
        "                                                        model_evaluation['Train_RMSPE'], model_evaluation['Test_RMSPE']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qAgymDpx6N"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMffxkwpx6N"
      },
      "source": [
        "Over here we have used GridSearchCV hyperparameter optimization technique , the reason for using this technique is that In order to train and score the model, grid Search creates a grid of hyperparameter values and chooses random combinations. As a result, we are able to specifically regulate the quantity of parameter combinations that are tried. Based on available time or resources, the number of search iterations is decided "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hykwinpx6N"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVzZC6opx6N"
      },
      "source": [
        "We have seen highly jump in the accuracy of the model by using the hyperparameter tuning our lasso r2 score increase to .82"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "Evaluation metrics are used in machine learning to measure the performance of a model on a given dataset. This allows us to compare the performance of different models and select the one that performs the best on the task at hand. we have used evaluation metrics for this tasks, mean squared error for regression problems , RMSE.Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "As we have seen above that, Random Forest Regressor is performing the best with the accuracy of 97.3% followed by Decison Tree Regressor with accuracy of 94.3%\n",
        "so here we are choosing Random Forest Regressor for best prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlJ88mOxoPgQ"
      },
      "outputs": [],
      "source": [
        "# print metrics table\n",
        "metrics_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "Random Forest is a supervised learning algorithm that can be used for both classification and regression tasks. A Random Forest regressor is a specific type of Random Forest that is used for regression tasks, which involve predicting a continuous output value (such as a price or temperature) rather than a discrete class label.\n",
        "\n",
        "The algorithm works by creating an ensemble of decision trees, where each tree is trained on a random subset of the data. The final output is then obtained by averaging the predictions of all the trees. This helps to reduce overfitting and improve the overall performance of the model.\n",
        "\n",
        "Random Forest regressor is known to be a very powerful algorithm that can handle high-dimensional data and a large number of input features. It is also relatively easy to use and interpret. It has several parameters that can be adjusted to optimize its performance, such as the number of trees in the ensemble, the maximum depth of each tree, and the minimum number of samples required to split a node.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLx3iX6929oX"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpfO2K1KqEiZ"
      },
      "outputs": [],
      "source": [
        "features = X.columns\n",
        "importances = random_forest_model.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.title('Feature Importances', fontsize=20)\n",
        "plt.barh(range(len(indices)), importances[indices], color='red', align='center')\n",
        "plt.yticks(range(len(indices)), features[indices])\n",
        "plt.xlabel('Relative Importance')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "ys8HaFFe7bsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Before creating the regression model, it is crucial to clean and pre-process the data so\n",
        " that it is in the right format.\n",
        "*   Unnecessary data that skew the results were also filtered out.\n",
        "\n",
        "\n",
        "*   Feature engineering and exploratory data analysis were performed to gather more meaningful information from the data.\n",
        "*   Apart from this, various data visualization, like box plot, frequency plot, histogram, pair plot, correlation matrix and scatter plot were created to understand the uni-variate distribution and multi-variate relationship of the data\n",
        "\n",
        "\n",
        "*   Majority of cars are Diesel (49.6%) and Petrol (48.9%)\n",
        "*   The number of unique car count is: 1491\n",
        "*   Maximum sold cars belongs to First owners.\n",
        "*   selling price of cars is decreasing as per their running status.\n",
        "*   we performed Regression Analysis on our data set to model the prices of cars\n",
        " \n",
        "**ML model**\n",
        "\n",
        "\n",
        "\n",
        "*   Among the all regression models, it is clear that Random Forest Regressor is giving the best result with the accuracy of 72.4% followed by linear Regressor with accuracy of 69.2%. So, we will use the random forest regressor to predict the sales.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vvUxYCoJ-yjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "OO0xa9c8-nNo"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}